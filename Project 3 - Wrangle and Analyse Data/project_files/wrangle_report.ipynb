{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The dataset that we wrangled is the tweet archive of Twitter user `@dog_rates`, also known as WeRateDogs. WeRateDogs is a Twitter account that rates peopleâ€™s dogs with a humorous comment about the dog. These ratings generally have a denominator of 10, and the numerators almost always greater than 10. The tweet archive records using in this project contains basic tweet data (tweet ID, timestamp, text, etc.) for all 2356 of their tweets as they stood on August 1, 2017.\n",
    "Installing Libraries\n",
    "\n",
    "We installed the following libraries using the Anaconda Powershell Prompt: Requests, Tweepy, and JSON then loaded the rest as such:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import string\n",
    "\n",
    "# Inline matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data for this Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Twitter Archive\n",
    "\n",
    "The WeRateDogs Twitter archive. We will manually save this file as: `twitter_archive_enhanced.csv`\n",
    "\n",
    "**URL:** https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image Predictions\n",
    "\n",
    "The tweet image predictions. We will programatically save this file as: `image_predictions.tsv` using the Requests library. \n",
    "\n",
    "**URL:** https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Twitter API\n",
    "\n",
    "Query using the Twitter API to obtain each tweet's retweet count and favorite (\"like\") count at minimum, and any additional data we find interesting. We will save this file as: `tweet_json.txt`. We accessed the API using this code before hiding our generated keys:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Accessing the Twitter API using our generated keys\n",
    "consumer_key = 'hidden'\n",
    "consumer_secret = 'hidden'\n",
    "access_token = 'hidden'\n",
    "access_secret = 'hidden'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# Obtain tweet status and assigning a variable 'tweet'\n",
    "tweet = api.get_status(twitter_archive.tweet_id[2000], tweet_mode='extended')\n",
    "\n",
    "# Obtain json status and assigning a variable 'status'\n",
    "status = tweet._json\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a loop to download the tweets we found while timestamping our requests with a counter, we also created code to ignore exception errors. The exceptions are printed into a dictionary called `dict_keys` for future reference."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Assigning variables for our loop\n",
    "tweet_errors = {}\n",
    "tweet_count = 1\n",
    "tweet_ids = twitter_archive.tweet_id\n",
    "\n",
    "# Using the datetime module to timestamp our requests\n",
    "(datetime.datetime.now().time())\n",
    "\n",
    "# Assigning an array variable to our data\n",
    "data = []\n",
    "\n",
    "# For loop for obtaining the tweets\n",
    "for tweet_id in tweet_ids:\n",
    "    try: # Try forces the loop to continue using the exception code at the bottom\n",
    "        \n",
    "        # Printing the id counter\n",
    "        print(tweet_count)\n",
    "        \n",
    "        # Collect tweet info\n",
    "        tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "        status = tweet._json\n",
    "        \n",
    "        # Append to file\n",
    "        data.append(status)\n",
    "        with open(file_name, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "            \n",
    "        # Print timer info to estimate time until wake-up\n",
    "        print(datetime.datetime.now().time())\n",
    "        \n",
    "        # Add one to the tweet count for further printing\n",
    "        tweet_count += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Print exception error information add to a dictionary variable named tweet_errors\n",
    "            \n",
    "        print(str(tweet_id) + \": \" + str(e))\n",
    "        tweet_errors[str(tweet_count - 1) + \"_\" + str(tweet_id)] = status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed both visual and programmatical assessments on the twitter_archive_en, img_pred and api_data dataframes and found these issues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Issues with `twitter_archive_en`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quality Issues:**\n",
    "\n",
    "1. Missing data such as ``NaN`` in the `in_reply_to_status_id` column\n",
    "2. Missing data such as `NaN` in the `in_reply_to_user_id` column\n",
    "3. Missing data such as `NaN` in the `retweeted_status_id` column\n",
    "4. Missing data such as `NaN` in the `retweeted_status_user_id` column\n",
    "5. Missing data such as `NaN` in the `retweeted_status_timestamp` column\n",
    "6. Missing  urls in `expanded_urls` column, used for images\n",
    "7. Missing names such as 'None' in the `name` column\n",
    "8. Incorrect names such as 'a' or 'the' in the `name` column\n",
    "9. Incorrect datatype `int64` in the `tweet_id` column\n",
    "10. Dog-types in individual columns with repeated and missing values\n",
    "\n",
    "**Tidiness Issues**\n",
    "\n",
    "1. Drop duplicate data `<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>` in the `source` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Issues with `img_pred`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quality Issues:**\n",
    "\n",
    "1. Incorrect datatype such as `int64` the in the `tweet_id` column\n",
    "2. `2330` entries vs `2356` in `twitter_archive_en.cvs` means that some posts will not have have predictions when we join the tables\n",
    "\n",
    "**Tidiness Issues**\n",
    "\n",
    "1. Columns `p1`, `p2` and `p3` all contain similar types of data for dog breed predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Issues with `api_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quality Issues:**\n",
    "\n",
    "1. Incorrect datatype such as `int64` the in the `tweet_id` column\n",
    "2. `2075` entries vs `2356` in `twitter_archive_en.cvs` means that some posts will not have have images when we join the tables\n",
    "\n",
    "**Tidiness Issues**\n",
    "\n",
    "1. This dataframe is not joined with the other tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we had assessed the data, we begun to clean it. But first, it is always good practice to make copies to preserve each dataframe:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Making copies of the dataframes\n",
    "archive_clean = twitter_archive_en.copy()\n",
    "img_pred_clean = img_pred.copy()\n",
    "api_data_clean = api_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we dropped the columns we did not find useful using `.drop`. We also marked the data with null values as NaN. The missing and incorrect names in the `name` columns where fixed by looking for names that weren't names as such:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creating a list of incorrect names to remove from the 'name' column\n",
    "remove_names = ['such', 'a', 'quite', 'not', 'one', 'incredibly', 'mad', 'an',\n",
    "       'very', 'just', 'my', 'his', 'actually', 'getting', 'this',\n",
    "       'unacceptable', 'all', 'old', 'infuriating', 'the', 'by',\n",
    "       'officially', 'life', 'light', 'space', 'None']\n",
    "\n",
    "# Removing the names from the list and replacing them with NaN\n",
    "archive_clean['name'] = archive_clean['name'].replace(remove_names, np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we changed the `int64` datatypes to string `object` datatypes. We merged the dog stages into a single column to clean up the dataframe. In the `img_pred_clean` dataframe we removed posts without images by using `.isin` to compare our columns to the other dataframes and remove them if they do not exist."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Comparing the tweet_id values of api_data_clean and img_pred to find total missing tweets\n",
    "no_tweet_api = -api_data_clean.tweet_id.isin(list(img_pred_clean.tweet_id))\n",
    "\n",
    "# Remove missing tweet_id values\n",
    "api_data_clean = api_data_clean[-no_tweet_api]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we matched the number of `tweet_id` entries of `archive_clean`, `api_data_clean` and `img_pred` using `isin`. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Comparing the tweet_id values of all datasets to 'api_data_clean' to find total missing tweets\n",
    "no_tweet_archive = -archive_clean.tweet_id.isin(list(api_data_clean.tweet_id))\n",
    "no_tweet_img = -img_pred_clean.tweet_id.isin(list(api_data_clean.tweet_id))\n",
    "\n",
    "# Remove missing tweet_id values\n",
    "archive_clean = archive_clean[-no_tweet_archive]\n",
    "img_pred_clean = img_pred_clean[-no_tweet_img]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we used `.pd.wide_to_long()` to reshape the p columns of the `img_pred_clean` dataframe and append similar data.\n",
    "Columns p1, p2 and p3 all contain similar types of data for dog breed predictions. After renaming the columns we converted the dataframe:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Converting the df from wide to long using 'pd.wide_to_long'\n",
    "img_pred_clean = pd.wide_to_long(img_pred_clean, \n",
    "                 stubnames = ['prediction', 'confidence', 'dog'], # The wide format variables.\n",
    "                 i = ['tweet_id', 'jpg_url', 'img_num'], # Column(s) to use as id variable(s), \n",
    "                 j='prediction_num', sep='_').reset_index() # The sub-observation variable in the long format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete our data wrangling, we merged the data from `api_data_clean` to `archive_clean` using `.pd.merge()` to merge `api_data_clean` dataframe to right of `archive_clean`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Merging the two dataframes and saving them to archive_clean\n",
    "archive_clean = pd.merge(left=archive_clean, right=api_data_clean, how='left', on='tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finishing touches included removing ratings and shortlinks from the `text` column in `archive_clean`. Use `re` to remove shortlinks from text column in `archive_clean` then remove the ratings."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Importing 're' library to find and remove urls then creating a function with it\n",
    "import re\n",
    "def remove_urls (url_delete):\n",
    "    url_delete = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', url_delete, flags=re.MULTILINE)\n",
    "    return(url_delete)\n",
    "\n",
    "# Applying the code 'remove_urls' to a variable named 'archive_clean.text' to modify the 'text' column\n",
    "archive_clean.text = archive_clean.text.apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeated a similar process to remove the ratings after removing the shortlinks then saved the files as `.csv`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Saving the files as .csv\n",
    "archive_clean.to_csv('twitter_archive_master.csv', index=False)\n",
    "img_pred_clean.to_csv('predictions_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
